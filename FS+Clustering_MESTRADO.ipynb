{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQvi/Y4BFrioukKp0/IJ0U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johkaizer/johkaizer.github.io/blob/master/FS%2BClustering_MESTRADO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JCKWfeyDlZGE"
      },
      "outputs": [],
      "source": [
        "#carregar dados\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df = pd.read_csv(\"/content/vendizap_esparsa.csv\", sep=';', encoding='ISO-8859-1', usecols=list(range(5, 104)))\n",
        "#df = pd.read_csv(\"/content/online_retail_II_matrix.csv\", sep=';', encoding='ISO-8859-1', nrows=10000)\n",
        "df2 = pd.read_csv(\"/content/vendizap_esparsa.csv\", sep=';', encoding='ISO-8859-1', usecols=list(range(2, 3)))\n",
        "# função para substituir vírgulas por pontos e converter para float\n",
        "def str_to_float(x):\n",
        "    if isinstance(x, str):\n",
        "        return float(x.replace(',', '.'))\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "def print_best_worst (scores, vezes=5):\n",
        "    scores = sorted(scores, reverse = True)\n",
        "    \n",
        "    print(f\"The {vezes} best features selected by this method are :\")\n",
        "    for i in range(vezes):\n",
        "        print(scores[i][1])\n",
        "    \n",
        "    print (f\"The {vezes} worst features selected by this method are :\")\n",
        "    for i in range(vezes):\n",
        "        print(scores[len(scores)-1-i][1])\n",
        "df = df.applymap(str_to_float).astype(float)\n",
        "df['Soma'] = df.sum(axis=1)\n",
        "df=df.fillna(0)\n",
        "X = df.drop(columns=[\"Soma\"]).values\n",
        "y = df2[\"Bairro\"].values\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula a matriz de correlação\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Verifica as correlações com a variável de destino\n",
        "correlations = corr_matrix['Soma'].drop('Soma')\n",
        "\n",
        "# Imprime as correlações\n",
        "#print(correlations[correlations > 0.1].sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "063cRsgXl0VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-genetic\n",
        "def elbow(importances, atributos):\n",
        "  # Ordenar as importâncias de forma decrescente\n",
        "  sorted_importances = sorted(zip(importances, atributos), reverse=True)\n",
        "  # Calcular a variância explicada acumulada para cada número de atributos mantidos\n",
        "  variances = []\n",
        "  total_variance = np.sum(importances)\n",
        "  cumulative_variance = 0\n",
        "  for i in range(len(atributos)):\n",
        "      var = sorted_importances[i][0]\n",
        "      cumulative_variance += var\n",
        "      variances.append(cumulative_variance / total_variance)\n",
        "\n",
        "  # Plotar a curva de variância em função do número de atributos mantidos\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(variances, 'o-')\n",
        "  ax.set_xlabel('Número de atributos')\n",
        "  ax.set_ylabel('Variância explicada acumulada')\n",
        "  plt.show()\n",
        "\n",
        "  # Selecionar o número de atributos que explique uma quantidade significativa da variância\n",
        "  threshold = 0.8\n",
        "  num_features = np.sum(np.array(variances) < threshold) + 1\n",
        "  selected_features = [f[1] for f in sorted_importances[:num_features]]\n",
        "\n",
        "  print(f'Foram selecionados {num_features} atributos: {selected_features}')\n",
        "  return num_features\n",
        "\n",
        "\n",
        "def getScores(importance, atributos):\n",
        "  scores = []\n",
        "  for i in range(len(df.drop(columns=[\"Soma\"]).columns)):\n",
        "      scores.append((importance[i],atributos[i]))\n",
        "  #print_best_worst(scores)\n",
        "  return scores\n",
        "\n",
        "\n",
        "def unsupervised(X, y, algoritmo='randomforestregressor'): \n",
        "  if algoritmo=='randomforestregressor':\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    rf = RandomForestRegressor()\n",
        "  elif algoritmo=='randomforestclassifier':\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    rf = RandomForestClassifier()\n",
        "  elif algoritmo=='lgbmregressor':\n",
        "    import lightgbm as lgb \n",
        "    rf = lgb.LGBMRegressor()\n",
        "  elif algoritmo=='lgbmclassifier':\n",
        "    import lightgbm as lgb \n",
        "    rf = lgb.LGBMClassifier()\n",
        "  elif algoritmo=='xgboostregressor':\n",
        "    from xgboost import XGBRegressor\n",
        "    rf = XGBRegressor(objective='reg:squarederror')\n",
        "  \n",
        "    \n",
        "  rf.fit(X, y)\n",
        "  # Obtendo as importâncias dos atributos\n",
        "  importances = rf.feature_importances_\n",
        "  atributos=df.drop(columns=[\"Soma\"]).columns\n",
        "  #Elbow para saber o numero de atributos a serem usados\n",
        "  elbow(importances, atributos)\n",
        "  return getScores(importances, atributos) #retorna os scores\n",
        "\n",
        "def others(X, y, algoritmo='svm'): #X=df.drop(columns=[\"Soma\"]).values\n",
        "  if algoritmo=='svm':\n",
        "    from sklearn import svm\n",
        "    rf = svm.SVC(kernel='linear')\n",
        "    rf.fit(X, y)\n",
        "    importances = pd.Series(abs(rf.coef_[0]), index=df.drop(columns=[\"Soma\"]).columns)\n",
        "  elif algoritmo=='selectkbest':\n",
        "    from sklearn.feature_selection import SelectKBest\n",
        "    from sklearn.feature_selection import chi2, mutual_info_classif\n",
        "    rf = SelectKBest(score_func=chi2, k='all')\n",
        "    rf.fit(X, y)\n",
        "    importances = pd.Series(rf.scores_, index=df.drop(columns=[\"Soma\"]).columns)\n",
        "  elif algoritmo=='rfe':\n",
        "    from sklearn.feature_selection import RFE\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    rf = RFE(LogisticRegression())\n",
        "    rf.fit(X, y)\n",
        "    importances = pd.Series(rf.ranking_, index=df.drop(columns=[\"Soma\"]).columns)\n",
        "  elif algoritmo=='genetico': ##Usar se tiver classes\n",
        "    from genetic_selection import GeneticSelectionCV\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
        "                                                    y, \n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "    scv = StratifiedKFold(n_splits=2)\n",
        "    # Definindo o classificador\n",
        "    clf = RandomForestClassifier()\n",
        "    selector = GeneticSelectionCV(clf,\n",
        "                                  cv=scv,\n",
        "                                  verbose=0,\n",
        "                                  scoring=\"f1_macro\",\n",
        "                                  max_features=15,\n",
        "                                  n_population=50,\n",
        "                                  crossover_proba=0.5,\n",
        "                                  mutation_proba=0.2,\n",
        "                                  n_generations=40,\n",
        "                                  crossover_independent_proba=0.5,\n",
        "                                  mutation_independent_proba=0.05,\n",
        "                                  tournament_size=3,\n",
        "                                  n_gen_no_change=10,\n",
        "                                  caching=True)\n",
        "    selector = selector.fit(X_train, y_train)\n",
        "    # get the selected features\n",
        "    cols = df.drop(columns=[\"Soma\"]).columns\n",
        "    selected_feats = [cols[i] for i in np.where(selector.support_)[0]]\n",
        "    print(selected_feats)\n",
        "    importances=selector.support_\n",
        "    print(\"<DESCONSIDERAR ELBOW E SCORES!!!>\")\n",
        "\n",
        "  elif algoritmo=='lasso':\n",
        "    from sklearn.feature_selection import SelectFromModel\n",
        "    from sklearn.linear_model import Lasso\n",
        "    rf = Lasso(alpha=0.15)\n",
        "    sfm = SelectFromModel(rf)\n",
        "    X_selected = sfm.fit_transform(X, y)\n",
        "    rf.fit(X_selected, y)\n",
        "    coeficientes = rf.coef_\n",
        "    # obtendo os nomes dos recursos selecionados\n",
        "    selected_features = np.where(sfm.get_support())[0]\n",
        "    feature_names = df.drop(columns=[\"Soma\"]).columns[selected_features]\n",
        "    selecionados=sorted(zip(feature_names, coeficientes), key=lambda x: abs(x[1]), reverse=True)\n",
        "    importances = [0] * len(df.drop(columns=[\"Soma\"]).columns)\n",
        "    for i, idx in enumerate(selected_features):\n",
        "        importances[idx] = coeficientes[i]\n",
        "  \n",
        "  atributos=df.drop(columns=[\"Soma\"]).columns\n",
        "  #Elbow para saber o numero de atributos a serem usados\n",
        "  elbow(importances, atributos)\n",
        "  return getScores(importances, atributos) #retorna os scores\n",
        "\n",
        "def criterio_kaiser(X):\n",
        "  ###Determinar numero de componentes: método do critério de Kaiser\n",
        "  # Calcula a matriz de covariância\n",
        "  cov_mat = np.cov(X.T)\n",
        "  # Calcula os autovalores e autovetores da matriz de covariância\n",
        "  eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
        "  # Ordena os autovalores em ordem decrescente\n",
        "  sorted_eig_vals = sorted(eig_vals, reverse=True)\n",
        "  # Aplica o critério de Kaiser\n",
        "  num_components = len([eig_val for eig_val in sorted_eig_vals if eig_val > 1])\n",
        "  print(\"Número de componentes pelo criterio de kaiser: \", num_components)\n",
        "  return num_components\n",
        "\n",
        "def pca(X, y, criterio='kaizer'):\n",
        "  from sklearn.decomposition import PCA\n",
        "  from sklearn.metrics import mean_squared_error\n",
        "\n",
        "  if criterio=='kaizer':\n",
        "    #Definindo o numero de componentes pelo criterio de Kaiser\n",
        "    num_components=criterio_kaiser(X)\n",
        "  elif criterio=='var':\n",
        "    ###Determinar numero de componentes: método do limiar\n",
        "    pca = PCA().fit(X)\n",
        "    explained_variance = pca.explained_variance_ratio_\n",
        "    cumulative_explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "    #Plotar\n",
        "    plt.plot(range(1, pca.n_components_ + 1), cumulative_explained_variance_ratio, marker='o')\n",
        "    plt.xlabel('Número de componentes principais')\n",
        "    plt.ylabel('Variância explicada cumulativa')\n",
        "    plt.show()\n",
        "    #Aplicar limiar\n",
        "    limite_variancia = 0.8\n",
        "    num_components=len([x for x in cumulative_explained_variance_ratio if x < limite_variancia])\n",
        "    print(f\"Número de componentes selecionado: {num_components}\")\n",
        "\n",
        "  # aplicando a técnica PCA para selecionar os atributos\n",
        "  pca = PCA(n_components=num_components)\n",
        "  X_selected = pca.fit_transform(X)\n",
        "  print(f\"Variancia explicada: {sum(pca.explained_variance_ratio_)}\")\n",
        "  return X_selected\n",
        "\n",
        "def ica(X, y):\n",
        "  from sklearn.decomposition import FastICA\n",
        "  import matplotlib.pyplot as plt\n",
        "  from scipy.stats import kurtosis\n",
        "\n",
        "  limiar=0.8\n",
        "  ica = FastICA(n_components=X.shape[1])\n",
        "  S_ = ica.fit_transform(X)\n",
        "  # calculando a variância explicada por cada componente\n",
        "  explained_variance = np.var(S_, axis=0)\n",
        "  # calculando a variância total\n",
        "  total_variance = np.sum(explained_variance)\n",
        "  # calculando a proporção de variância explicada acumulada por cada componente\n",
        "  explained_variance_ratio = explained_variance / total_variance\n",
        "  # calculando a variância explicada acumulada\n",
        "  cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "  # plotando a curva de variância explicada acumulada\n",
        "  import matplotlib.pyplot as plt\n",
        "  plt.plot(range(1, X.shape[1]+1), cumulative_variance_ratio)\n",
        "  plt.xlabel('Número de componentes')\n",
        "  plt.ylabel('Variância explicada acumulada')\n",
        "  plt.show()\n",
        "  # selecionando o número de componentes que explicam, pelo menos, limiar% da variância\n",
        "  num_components = np.argmax(cumulative_variance_ratio >= limiar) + 1\n",
        "  print(f\"Número de componentes selecionado: {num_components}\")\n",
        "\n",
        "  # aplicando a técnica PCA para selecionar os atributos\n",
        "  ica = FastICA(n_components=num_components)\n",
        "  X_selected = ica.fit_transform(X)\n",
        "  return X_selected\n",
        "\n",
        "def nmf(X, y): #X=df.drop(columns=[\"Soma\"]).values\n",
        "  from sklearn.decomposition import NMF\n",
        "  import matplotlib.pyplot as plt\n",
        "  from scipy.stats import kurtosis\n",
        "\n",
        "  limiar=0.8\n",
        "  nmf = NMF(n_components=X.shape[1])\n",
        "  S_ = nmf.fit_transform(X)\n",
        "  # calculando a variância explicada por cada componente\n",
        "  explained_variance = np.var(S_, axis=0)\n",
        "  # calculando a variância total\n",
        "  total_variance = np.sum(explained_variance)\n",
        "  # calculando a proporção de variância explicada acumulada por cada componente\n",
        "  explained_variance_ratio = explained_variance / total_variance\n",
        "  # calculando a variância explicada acumulada\n",
        "  cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "  # plotando a curva de variância explicada acumulada\n",
        "  import matplotlib.pyplot as plt\n",
        "  plt.plot(range(1, X.shape[1]+1), cumulative_variance_ratio)\n",
        "  plt.xlabel('Número de componentes')\n",
        "  plt.ylabel('Variância explicada acumulada')\n",
        "  plt.show()\n",
        "  # selecionando o número de componentes que explicam, pelo menos, limiar% da variância\n",
        "  num_components = np.argmax(cumulative_variance_ratio >= limiar) + 1\n",
        "  print(f\"Número de componentes selecionado: {num_components}\")\n",
        "  \n",
        "  # aplicando a técnica nmf para selecionar os atributos\n",
        "  nmf = NMF(n_components=num_components)\n",
        "  X_selected = nmf.fit_transform(X)\n",
        "  return X_selected\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqr8hdIOl9Xq",
        "outputId": "793b45d4-dcea-420f-9ace-df2e310a7c50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn-genetic\n",
            "  Downloading sklearn_genetic-0.5.1-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.23 in /usr/local/lib/python3.8/dist-packages (from sklearn-genetic) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sklearn-genetic) (1.22.4)\n",
            "Collecting deap>=1.0.2\n",
            "  Downloading deap-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.23->sklearn-genetic) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.23->sklearn-genetic) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.23->sklearn-genetic) (1.7.3)\n",
            "Requirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.8/dist-packages (from multiprocess->sklearn-genetic) (0.3.6)\n",
            "Installing collected packages: multiprocess, deap, sklearn-genetic\n",
            "Successfully installed deap-1.3.3 multiprocess-0.70.14 sklearn-genetic-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans(X):\n",
        "  from sklearn.cluster import KMeans\n",
        "  from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "  import numpy as np\n",
        "\n",
        "  # Define o intervalo de número de clusters para testar\n",
        "  n_clusters_range = range(2, X.shape[1])\n",
        "\n",
        "  # Inicializa as listas para armazenar os resultados\n",
        "  silhouette_scores = []\n",
        "  calinski_harabasz_scores = []\n",
        "  sse = []\n",
        "\n",
        "  # Itera sobre o número de clusters para testar\n",
        "  for n_clusters in n_clusters_range:\n",
        "      # Executa o algoritmo Sparse K-Means\n",
        "      kmeans = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=100, n_init=1)\n",
        "      kmeans.fit(X)\n",
        "      labels = kmeans.labels_\n",
        "      \n",
        "      # Calcula os scores de silhouette e calinski-harabasz\n",
        "      s_score = silhouette_score(X, labels, metric='cosine')\n",
        "      ch_score = calinski_harabasz_score(X, labels)\n",
        "      \n",
        "      # Adiciona os scores nas listas\n",
        "      silhouette_scores.append(s_score)\n",
        "      calinski_harabasz_scores.append(ch_score)\n",
        "\n",
        "      # Calcula o erro quadrático médio e a variância explicada acumulada\n",
        "      sse.append(kmeans.inertia_)\n",
        "\n",
        "\n",
        "  # Encontra o número de clusters ideal\n",
        "  s_best_idx = np.argmax(silhouette_scores)\n",
        "  ch_best_idx = np.argmax(calinski_harabasz_scores)\n",
        "  sse_best_idx = np.argmin(sse)\n",
        "\n",
        "  # Imprime o número de clusters ideais e os scores correspondentes\n",
        "  print(f'Número ideal de clusters (silhouette score): {n_clusters_range[s_best_idx]}, Score: {silhouette_scores[s_best_idx]}')\n",
        "  print(f'Número ideal de clusters (calinski-harabasz score): {n_clusters_range[ch_best_idx]}, Score: {calinski_harabasz_scores[ch_best_idx]}')\n",
        "  print(f'Número ideal de clusters (SSE): {n_clusters_range[sse_best_idx]}, Score: {sse[sse_best_idx]}')\n",
        "\n",
        "  kmeans = KMeans(n_clusters=n_clusters_range[s_best_idx], init='k-means++')\n",
        "  kmeans.fit(X)\n",
        "  labels = kmeans.labels_\n",
        "\n",
        "  ext=pd.DataFrame(labels)\n",
        "  ext.to_excel(\"outputK.xlsx\")\n",
        "\n",
        "def dbscan(X):\n",
        "    from sklearn.cluster import DBSCAN\n",
        "    from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "    import numpy as np\n",
        "\n",
        "    # Define o intervalo de parâmetros para testar\n",
        "    eps_range = np.linspace(0.1, 1.0, 10)\n",
        "    min_samples_range = range(2, X.shape[1])\n",
        "\n",
        "    # Inicializa as listas para armazenar os resultados\n",
        "    silhouette_scores = []\n",
        "    calinski_harabasz_scores = []\n",
        "\n",
        "    # Itera sobre os valores de eps e min_samples para testar\n",
        "    for eps in eps_range:\n",
        "        for min_samples in min_samples_range:\n",
        "            # Executa o algoritmo DBSCAN\n",
        "            dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')\n",
        "            dbscan.fit(X)\n",
        "            labels = dbscan.labels_\n",
        "\n",
        "            # Verifica se o número de clusters é maior que 1 para calcular os scores\n",
        "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "            if n_clusters > 1:\n",
        "                # Calcula os scores de silhouette e calinski-harabasz\n",
        "                s_score = silhouette_score(X, labels, metric='cosine')\n",
        "                ch_score = calinski_harabasz_score(X, labels)\n",
        "\n",
        "                # Adiciona os scores nas listas\n",
        "                silhouette_scores.append(s_score)\n",
        "                calinski_harabasz_scores.append(ch_score)\n",
        "    # Encontra os parâmetros ideais\n",
        "    s_best_idx = np.argmax(silhouette_scores)\n",
        "    ch_best_idx = np.argmax(calinski_harabasz_scores)\n",
        "\n",
        "    # Imprime os parâmetros ideais e os scores correspondentes\n",
        "    print(f'Parâmetros ideais (silhouette score): eps={eps_range[s_best_idx//len(min_samples_range)],} min_samples={min_samples_range[s_best_idx%len(min_samples_range)]}, Score: {silhouette_scores[s_best_idx]}')\n",
        "    print(f'Parâmetros ideais (calinski-harabasz score): eps={eps_range[ch_best_idx//len(min_samples_range)],} min_samples={min_samples_range[ch_best_idx%len(min_samples_range)]}, Score: {calinski_harabasz_scores[ch_best_idx]}')\n",
        "\n",
        "    dbscan = DBSCAN(eps=eps_range[s_best_idx//len(min_samples_range)], min_samples=min_samples_range[s_best_idx%len(min_samples_range)], metric='cosine')\n",
        "    dbscan.fit(X)\n",
        "    labels= dbscan.labels_\n",
        "    # Imprime o número de clusters e os scores correspondentes\n",
        "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    print(f'Número de clusters: {n_clusters}') \n",
        "\n",
        "    ext=pd.DataFrame(labels)\n",
        "    ext.to_excel(\"outputD.xlsx\")\n",
        "\n",
        "    return labels\n"
      ],
      "metadata": {
        "id": "q9K9D54z-HR5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hierarchical_clustering(X):\n",
        "    from sklearn.cluster import AgglomerativeClustering\n",
        "    from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "    # Define o intervalo de número de clusters para testar\n",
        "    n_clusters_range = range(2, X.shape[1])\n",
        "\n",
        "    # Inicializa as listas para armazenar os resultados\n",
        "    silhouette_scores = []\n",
        "    calinski_harabasz_scores = []\n",
        "\n",
        "    # Itera sobre o número de clusters para testar\n",
        "    for n_clusters in n_clusters_range:\n",
        "        # Executa o algoritmo Hierarchical Clustering\n",
        "        hc = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "        hc.fit(X)\n",
        "        labels = hc.labels_\n",
        "\n",
        "        # Calcula os scores de silhouette e calinski-harabasz\n",
        "        s_score = silhouette_score(X, labels, metric='cosine')\n",
        "        ch_score = calinski_harabasz_score(X, labels)\n",
        "\n",
        "        # Adiciona os scores nas listas\n",
        "        silhouette_scores.append(s_score)\n",
        "        calinski_harabasz_scores.append(ch_score)\n",
        "\n",
        "    # Encontra o número de clusters ideal\n",
        "    s_best_idx = np.argmax(silhouette_scores)\n",
        "    ch_best_idx = np.argmax(calinski_harabasz_scores)\n",
        "\n",
        "    # Imprime o número de clusters ideais e os scores correspondentes\n",
        "    print(f'Número ideal de clusters (silhouette score): {n_clusters_range[s_best_idx]}, Score: {silhouette_scores[s_best_idx]}')\n",
        "    print(f'Número ideal de clusters (calinski-harabasz score): {n_clusters_range[ch_best_idx]}, Score: {calinski_harabasz_scores[ch_best_idx]}')\n",
        "\n",
        "    hc = AgglomerativeClustering(n_clusters=n_clusters_range[s_best_idx])\n",
        "    hc.fit(X)\n",
        "    labels = hc.labels_\n",
        "\n",
        "    ext=pd.DataFrame(labels)\n",
        "    ext.to_excel(\"outputH.xlsx\")\n"
      ],
      "metadata": {
        "id": "sTII6cIIaa6B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def espectral(X):\n",
        "  from sklearn.cluster import SpectralClustering\n",
        "  from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "  from sklearn.preprocessing import normalize\n",
        "\n",
        "  # Define o intervalo de número de clusters para testar\n",
        "  n_clusters_range = range(2, X.shape[1])\n",
        "\n",
        "  # Inicializa as listas para armazenar os resultados\n",
        "  silhouette_scores = []\n",
        "  calinski_harabasz_scores = []\n",
        "\n",
        "  # Itera sobre o número de clusters para testar\n",
        "  for n_clusters in n_clusters_range:\n",
        "      # Executa o algoritmo de Clustering Espectral\n",
        "      spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors')\n",
        "      spectral.fit(X)\n",
        "      labels = spectral.labels_\n",
        "      \n",
        "      # Calcula o score de silhouette\n",
        "      s_score = silhouette_score(X, labels)\n",
        "      ch_score = calinski_harabasz_score(X, labels)\n",
        "      \n",
        "      # Adiciona o score na lista\n",
        "      silhouette_scores.append(s_score)\n",
        "      calinski_harabasz_scores.append(ch_score)\n",
        "\n",
        "  # Encontra o número de clusters ideal\n",
        "  s_best_idx = np.argmax(silhouette_scores)\n",
        "  ch_best_idx = np.argmax(calinski_harabasz_scores)\n",
        "\n",
        "  # Imprime o número de clusters ideais e os scores correspondentes\n",
        "  print(f'Número ideal de clusters (silhouette score): {n_clusters_range[s_best_idx]}, Score: {silhouette_scores[s_best_idx]}')\n",
        "  print(f'Número ideal de clusters (calinski-harabasz score): {n_clusters_range[ch_best_idx]}, Score: {calinski_harabasz_scores[ch_best_idx]}')\n",
        "  \n",
        "  spectral = SpectralClustering(n_clusters=n_clusters_range[best_idx], affinity='nearest_neighbors')\n",
        "  spectral.fit(X)\n",
        "  labels = spectral.labels_\n",
        "\n",
        "  ext=pd.DataFrame(labels)\n",
        "  ext.to_excel(\"outputE.xlsx\")\n",
        "\n"
      ],
      "metadata": {
        "id": "u-mbMvzfAT8t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fuzzy(X):\n",
        "  !pip install fuzzy-c-means\n",
        "  from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "  from sklearn.preprocessing import normalize\n",
        "  from sklearn.utils.extmath import randomized_svd\n",
        "  import numpy as np\n",
        "  from fcmeans import FCM\n",
        "\n",
        "  # Define o intervalo de número de clusters para testar\n",
        "  n_clusters_range = range(2, X.shape[1])\n",
        "\n",
        "  # Inicializa as listas para armazenar os resultados\n",
        "  silhouette_scores = []\n",
        "  calinski_harabasz_scores = []\n",
        "\n",
        "  # Itera sobre o número de clusters para testar\n",
        "  for n_clusters in n_clusters_range:\n",
        "      # Executa o algoritmo Fuzzy C-Means\n",
        "      fcm = FCM(n_clusters=n_clusters)\n",
        "      fcm.fit(X)\n",
        "      labels = fcm.u.argmax(axis=1)\n",
        "      \n",
        "      # Calcula os scores de silhouette e calinski-harabasz\n",
        "      s_score = silhouette_score(X, labels)\n",
        "      ch_score = calinski_harabasz_score(X, labels)\n",
        "      \n",
        "      # Adiciona os scores nas listas\n",
        "      silhouette_scores.append(s_score)\n",
        "      calinski_harabasz_scores.append(ch_score)\n",
        "\n",
        "  # Encontra o número de clusters ideal\n",
        "  s_best_idx = np.argmax(silhouette_scores)\n",
        "  ch_best_idx = np.argmax(calinski_harabasz_scores)\n",
        "\n",
        "  # Imprime o número de clusters ideais e os scores correspondentes\n",
        "  print(f'Número ideal de clusters (silhouette score): {n_clusters_range[s_best_idx]}, Score: {silhouette_scores[s_best_idx]}')\n",
        "  print(f'Número ideal de clusters (calinski-harabasz score): {n_clusters_range[ch_best_idx]}, Score: {calinski_harabasz_scores[ch_best_idx]}')\n",
        "\n",
        "  fcm = FCM(n_clusters=n_clusters_range[s_best_idx])\n",
        "  fcm.fit(X)\n",
        "  labels = fcm.u.argmax(axis=1)\n",
        "\n",
        "  fcm_labels = fcm.predict(X)\n",
        "  ext=pd.DataFrame(fcm_labels)\n",
        "  ext.to_excel(\"outputF.xlsx\")\n"
      ],
      "metadata": {
        "id": "6StQivXnCaye"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}